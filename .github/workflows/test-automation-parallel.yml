# ===============================================
# Parallel Test Automation Pipeline
# ===============================================

name: 🤖 Automated Test Results

on:
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  # ============================================
  # JOB 1: Web Testing
  # ============================================
  test-web:
    runs-on: ubuntu-latest
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4
      
      - name: 📥 Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12.4'
      
      - name: 📥 Install dependencies
        run: |
          pip install -r requirements.txt
      
      - name: 🧪 Run Web Tests
        run: |
          mkdir -p results/web
          robot \
            --variable BROWSER:headlesschrome \
            --outputdir results/web \
            --output output.xml \
            --log log.html \
            --report report.html \
            --name "Web Tests" \
            TestScript/Web/Staging/FlowLogin.robot || true
      
      - name: 📊 Generate Web Summary
        if: always()
        run: |
          cat > analyze_web.py << 'EOF'
          import xml.etree.ElementTree as ET
          import json
          
          tree = ET.parse('results/web/output.xml')
          root = tree.getroot()
          stats = root.find('.//statistics/total/stat')
          
          passed = int(stats.get('pass', 0))
          failed = int(stats.get('fail', 0))
          total = passed + failed
          
          summary = {
              "type": "web",
              "passed": passed,
              "failed": failed,
              "total": total,
              "pass_rate": round((passed/total*100), 2) if total > 0 else 0
          }
          
          with open('results/web/summary.json', 'w') as f:
              json.dump(summary, f, indent=2)
          
          print(f"✅ Web Tests: {passed}/{total} passed ({summary['pass_rate']}%)")
          EOF
          
          python analyze_web.py
      
      - name: 📤 Upload Web Results
        uses: actions/upload-artifact@v4
        with:
          name: web-results
          path: results/web/

  # ============================================
  # JOB 2: API Testing
  # ============================================
  test-api:
    runs-on: ubuntu-latest
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4
      
      - name: 📥 Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12.4'
      
      - name: 📥 Install dependencies
        run: |
          pip install -r requirements.txt
      
      - name: 🧪 Run API Tests
        run: |
          mkdir -p results/api
          robot \
            --outputdir results/api \
            --output output.xml \
            --log log.html \
            --report report.html \
            --name "API Tests" \
            TestScript/API/API.robot || true
      
      - name: 📊 Generate API Summary
        if: always()
        run: |
          cat > analyze_api.py << 'EOF'
          import xml.etree.ElementTree as ET
          import json
          
          tree = ET.parse('results/api/output.xml')
          root = tree.getroot()
          stats = root.find('.//statistics/total/stat')
          
          passed = int(stats.get('pass', 0))
          failed = int(stats.get('fail', 0))
          total = passed + failed
          
          summary = {
              "type": "api",
              "passed": passed,
              "failed": failed,
              "total": total,
              "pass_rate": round((passed/total*100), 2) if total > 0 else 0
          }
          
          with open('results/api/summary.json', 'w') as f:
              json.dump(summary, f, indent=2)
          
          print(f"✅ API Tests: {passed}/{total} passed ({summary['pass_rate']}%)")
          EOF
          
          python analyze_api.py
      
      - name: 📤 Upload API Results
        uses: actions/upload-artifact@v4
        with:
          name: api-results
          path: results/api/

  # ============================================
  # JOB 3: Combined Summary Report
  # ============================================
  summary:
    needs: [test-web, test-api]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: 📥 Download Web Results
        uses: actions/download-artifact@v4
        with:
          name: web-results
          path: web-results/
      
      - name: 📥 Download API Results
        uses: actions/download-artifact@v4
        with:
          name: api-results
          path: api-results/
      
      - name: 📊 Generate Combined Report
        run: |
          cat > generate_report.py << 'EOF'
          import json
          import xml.etree.ElementTree as ET
          import os
          from datetime import datetime
          
          def parse_xml(xml_path, test_type):
              """Parse Robot Framework XML and extract test details"""
              if not os.path.exists(xml_path):
                  print(f"⚠️ {test_type} results not found at {xml_path}")
                  return None
                  
              try:
                  tree = ET.parse(xml_path)
                  root = tree.getroot()
                  
                  stats = root.find('.//statistics/total/stat')
                  passed = int(stats.get('pass', 0))
                  failed = int(stats.get('fail', 0))
                  total = passed + failed
                  
                  # Extract test details
                  passed_tests = []
                  failed_tests = []
                  
                  for suite in root.findall('.//suite'):
                      for test in suite.findall('.//test'):
                          name = test.get('name', 'Unknown')
                          status = test.find('./status').get('status')
                          
                          if status == 'PASS':
                              passed_tests.append(name)
                          elif status == 'FAIL':
                              msg_node = test.find('./status')
                              error_msg = ""
                              if msg_node is not None and msg_node.text:
                                  error_msg = msg_node.text.strip().replace('\n', ' ')[:100]
                              failed_tests.append({
                                  'name': name,
                                  'error': error_msg
                              })
                  
                  return {
                      'passed': passed,
                      'failed': failed,
                      'total': total,
                      'pass_rate': round((passed/total*100), 2) if total > 0 else 0,
                      'passed_tests': passed_tests,
                      'failed_tests': failed_tests
                  }
              except Exception as e:
                  print(f"❌ Error parsing {test_type} results: {e}")
                  return None

          # Parse both test results
          web_results = parse_xml('web-results/output.xml', 'Web')
          api_results = parse_xml('api-results/output.xml', 'API')

          # Calculate combined statistics
          total_passed = (web_results['passed'] if web_results else 0) + (api_results['passed'] if api_results else 0)
          total_failed = (web_results['failed'] if web_results else 0) + (api_results['failed'] if api_results else 0)
          total_tests = total_passed + total_failed
          overall_pass_rate = round((total_passed/total_tests*100), 2) if total_tests > 0 else 0

          # Generate Markdown report
          with open('test_summary.md', 'w', encoding='utf-8') as f:
              # Header with overall status
              if total_tests == 0:
                  f.write("# ⚠️ No Test Results Found\n\n")
                  f.write("Please check the test configuration and logs.\n")
              elif overall_pass_rate == 100:
                  f.write("# ✅ All Tests Passed!\n\n")
              elif overall_pass_rate >= 80:
                  f.write("# 🎯 Tests Mostly Passed\n\n")
              elif overall_pass_rate >= 50:
                  f.write("# ⚠️ Some Tests Failed\n\n")
              else:
                  f.write("# ❌ Many Tests Failed\n\n")
              
              # Overall Summary
              f.write("## 📊 Overall Summary\n\n")
              f.write(f"**Total Pass Rate:** {overall_pass_rate}% ({total_passed}/{total_tests})\n\n")
              
              # Test Breakdown by Type with Test Cases
              f.write("## 📈 Test Breakdown by Type\n\n")
              f.write("| Test Type | Passed | Failed | Total | Pass Rate | Test Cases |\n")
              f.write("|:----------|:------:|:------:|:-----:|----------:|:-----------|")
              f.write("\n")
              
              # Web Tests Row with test cases
              if web_results:
                  # Format test cases for table cell
                  test_cases_list = []
                  
                  if web_results['passed_tests']:
                      test_cases_list.append("**✅ Passed:**")
                      for test in web_results['passed_tests']:
                          test_cases_list.append(f"• {test}")
                  
                  if web_results['failed_tests']:
                      if test_cases_list:
                          test_cases_list.append("")  # Empty line between sections
                      test_cases_list.append("**❌ Failed:**")
                      for test in web_results['failed_tests']:
                          test_cases_list.append(f"• {test['name']}")
                  
                  # Join with <br> for HTML line breaks in GitHub Markdown
                  test_cases_str = "<br>".join(test_cases_list) if test_cases_list else "No tests"
                  
                  f.write(f"| **Web Tests** | {web_results['passed']} | {web_results['failed']} | ")
                  f.write(f"{web_results['total']} | **{web_results['pass_rate']}%** | {test_cases_str} |\n")
              else:
                  f.write("| **Web Tests** | - | - | - | **N/A** | No tests executed |\n")
              
              # API Tests Row with test cases
              if api_results:
                  # Format test cases for table cell
                  test_cases_list = []
                  
                  if api_results['passed_tests']:
                      test_cases_list.append("**✅ Passed:**")
                      for test in api_results['passed_tests']:
                          test_cases_list.append(f"• {test}")
                  
                  if api_results['failed_tests']:
                      if test_cases_list:
                          test_cases_list.append("")  # Empty line between sections
                      test_cases_list.append("**❌ Failed:**")
                      for test in api_results['failed_tests']:
                          test_cases_list.append(f"• {test['name']}")
                  
                  # Join with <br> for HTML line breaks
                  test_cases_str = "<br>".join(test_cases_list) if test_cases_list else "No tests"
                  
                  f.write(f"| **API Tests** | {api_results['passed']} | {api_results['failed']} | ")
                  f.write(f"{api_results['total']} | **{api_results['pass_rate']}%** | {test_cases_str} |\n")
              else:
                  f.write("| **API Tests** | - | - | - | **N/A** | No tests executed |\n")
              
              # Total Row
              total_status = '✅' if overall_pass_rate == 100 else '⚠️' if overall_pass_rate >= 50 else '❌'
              all_test_count = 0
              
              # Count all tests for summary
              if web_results:
                  all_test_count += len(web_results['passed_tests']) + len(web_results['failed_tests'])
              if api_results:
                  all_test_count += len(api_results['passed_tests']) + len(api_results['failed_tests'])
              
              f.write(f"| **TOTAL** | **{total_passed}** | **{total_failed}** | **{total_tests}** | ")
              f.write(f"**{overall_pass_rate}%** | {total_status} Total {all_test_count} test cases |\n\n")
              
              # Failed Tests Details Section (if any) - Optional expanded view
              all_failed = []
              if web_results:
                  all_failed.extend([{'type': 'Web', **t} for t in web_results['failed_tests']])
              if api_results:
                  all_failed.extend([{'type': 'API', **t} for t in api_results['failed_tests']])
              
              if all_failed:
                  f.write("## ❌ Failed Tests Details\n\n")
                  f.write("<details>\n")
                  f.write("<summary>Click to view error details</summary>\n\n")
                  f.write("| # | Type | Test Name | Error Message |\n")
                  f.write("|:--|:-----|:----------|:--------------|\n")
                  for i, test in enumerate(all_failed, 1):
                      error = test['error'][:60] + "..." if len(test['error']) > 60 else test['error']
                      error = error.replace('|', '\\|').replace('\n', ' ')
                      f.write(f"| {i} | {test['type']} | {test['name']} | {error} |\n")
                  f.write("\n</details>\n\n")
              
              # Execution Details
              f.write("## 📝 Execution Details\n\n")
              f.write(f"- **Execution Time:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}\n")
              f.write(f"- **Branch:** ${{ github.ref_name }}\n")
              f.write(f"- **Commit:** ${{ github.sha }}\n")
              f.write(f"- **Triggered by:** @${{ github.actor }}\n")
              f.write(f"- **Workflow:** ${{ github.workflow }}\n")

          # Console output for debugging
          print("\n" + "="*60)
          print("📊 TEST EXECUTION SUMMARY")
          print("="*60)
          print(f"Overall Pass Rate: {overall_pass_rate}% ({total_passed}/{total_tests})")
          print("-"*60)

          if web_results:
              print(f"\n🌐 Web Tests: {web_results['pass_rate']}% ({web_results['passed']}/{web_results['total']})")
              if web_results['passed_tests']:
                  print("  ✅ Passed:")
                  for test in web_results['passed_tests']:
                      print(f"     - {test}")
              if web_results['failed_tests']:
                  print("  ❌ Failed:")
                  for test in web_results['failed_tests']:
                      print(f"     - {test['name']}")

          if api_results:
              print(f"\n🔌 API Tests: {api_results['pass_rate']}% ({api_results['passed']}/{api_results['total']})")
              if api_results['passed_tests']:
                  print("  ✅ Passed:")
                  for test in api_results['passed_tests']:
                      print(f"     - {test}")
              if api_results['failed_tests']:
                  print("  ❌ Failed:")
                  for test in api_results['failed_tests']:
                      print(f"     - {test['name']}")

          print("="*60)
          print("\n✅ Report generated successfully!")
          
          python generate_report.py
      
      - name: 📝 Update GitHub Job Summary
        if: always()
        run: |
          if [ -f "test_summary.md" ]; then
            cat test_summary.md >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ No test summary available" >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: 🗓️ Set current date and time
        id: datetime
        run: echo "datetime=$(TZ='Asia/Bangkok' date +'%Y-%m-%d_%H-%M-%S')" >> $GITHUB_OUTPUT
      
      - name: 📦 Create Combined Artifact
        if: always()
        run: |
          mkdir -p combined-results
          cp -r web-results/* combined-results/web/ 2>/dev/null || true
          cp -r api-results/* combined-results/api/ 2>/dev/null || true
          cp test_summary.md combined-results/ 2>/dev/null || true
      
      - name: 📤 Upload Combined Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: Test-Results-${{ steps.datetime.outputs.datetime }}
          path: combined-results/
          retention-days: 30
      